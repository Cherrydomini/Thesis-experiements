{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bigdatasvm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3lLMFoTZNJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541de635-e3f6-4c00-ef2c-0180b6d8d2d0"
      },
      "source": [
        "!pip install lime\n",
        "!pip install lifelines"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.7/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.16.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (0.22.2.post1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.5.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.0.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: lifelines in /usr/local/lib/python3.7/dist-packages (0.26.0)\n",
            "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.7/dist-packages (from lifelines) (0.5.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.19.5)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (3.2.2)\n",
            "Requirement already satisfied: formulaic<0.3,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from lifelines) (0.2.3)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.1.5)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->lifelines) (0.16.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (0.10.0)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (0.8.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (1.12.1)\n",
            "Requirement already satisfied: interface-meta>=1.2 in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (1.2.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->lifelines) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0->lifelines) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV0oTlvTFbv4"
      },
      "source": [
        " **Using LIME on the COMPAS dataset**\n",
        "\n",
        "In this experiment we used the LIME model explanation tool to understand the results given by the model. the model inquestion is the Non-Linear Support Vector Machine model. the dataset used in this model is the COMPAS dataset provided by propublica's github.\n",
        "\n",
        "First import the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TBdz8DiZOsR"
      },
      "source": [
        "from __future__ import print_function\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import sklearn.ensemble\n",
        "import sklearn.metrics\n",
        "from lime import lime_text\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import sklearn\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import linear_model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "import sklearn.feature_extraction \n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "import scipy.stats as st\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAbkD06NGxN0"
      },
      "source": [
        "Import the dataset\n",
        "\n",
        "Choose the target features and set it to the variable 'predict'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAkG79LjGxgt"
      },
      "source": [
        "compas = pd.read_csv(\"compas-scores.csv\")\n",
        "\n",
        "print(\"Charge degrees and recidivation outcomes:\", compas[[\"c_charge_degree\", \"is_recid\"]].value_counts())\n",
        "predict = 'is_recid'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTxphV1xHgxj"
      },
      "source": [
        "**Error 1**\n",
        "\n",
        " dataset has wrong data in the form of extra categories in the features 'is_recid' and 'c_charge_degree'. These features should only have two outcomes [0,1] and ['M','F'] respectfully. however each features has one extra outcome, -1, and ). First we need to find out how many of these extra features there are and how they are related to one another in these two categories.\n",
        "\n",
        "after confirming their existence and the amount i decided the amount was too significant to just have them deleted, and because they were not counted as recidivated it was a clerical error on the part of the office were the person was arrested but not convicted of a crime. Because of this i set the -1 value to 0 and the O value to M."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x4fj7RVHhBR"
      },
      "source": [
        "os = []\n",
        "total = len(compas)\n",
        "for i in range(total):\n",
        "  if compas[predict][i] == -1:\n",
        "    os.append(compas.iloc[i])\n",
        "    compas[predict][i] = 0\n",
        "    print(compas[predict][i])\n",
        "  if compas[\"c_charge_degree\"][i] == \"O\":\n",
        "    os.append(compas.iloc[i])\n",
        "    compas[\"c_charge_degree\"][i] = 'M'\n",
        "    print(compas[\"c_charge_degree\"][i])\n",
        "os = pd.DataFrame(os, columns =['sex','age','race','juv_fel_count','decile_score','juv_misd_count','juv_other_count','priors_count','c_charge_degree','v_decile_score','is_recid'] )\n",
        "print(\"number of -1: \", compas[\"is_recid\"].value_counts())\n",
        "print(\"recidivate decision in Os: \", o[\"is_recid\"].value_counts())\n",
        "print(\"O's: \", os[\"c_charge_degree\"].value_counts())\n",
        "print(compas[[\"c_charge_degree\", \"is_recid\"]].value_counts())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmW5TWj9PONg"
      },
      "source": [
        "Next, there are features that are important to the models decision making process, however they are not numbers, but rather they are string values. To make these values useable by the model we have to turn them into integers. This is a classifcation model, and there are only two values for each feature, so we will be making each faeture either 0 or a 1 for the 'sex' features and the 'c_charge_degree' feature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj_GwTwwPObN"
      },
      "source": [
        "#Encode sex values\n",
        "\n",
        "print(\"Sex values as string:\", compas['sex'].value_counts())\n",
        "for f in range(total):\n",
        "  if compas['sex'][f] == 'Male':\n",
        "    compas['sex'][f] = 0\n",
        "  if compas['sex'][f] == 'Female':\n",
        "    compas['sex'][f] = 1\n",
        "print(\"SEX VALUES AS BINARY:\",compas['sex'].value_counts())\n",
        "#Encode charge degree values\n",
        "print(\"CHARGE VALUES AS STRING: \", compas['c_charge_degree'].value_counts())\n",
        "for j in range(total):\n",
        "  if compas['c_charge_degree'][j] == 'M':\n",
        "    compas['c_charge_degree'][j] = 0\n",
        "  if compas['c_charge_degree'][j] == 'F':\n",
        "    compas['c_charge_degree'][j] = 1\n",
        "print(\"CHARGE VALUES AS BINARY: \", compas['c_charge_degree'].value_counts())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77pObiGzelJ7"
      },
      "source": [
        "**Error 2\n",
        "\n",
        "The other error that we will fix was actually found by another author. This was that the time frame did not quite match up for both recidivate and non-recidivate defendants. For the data to be within the valid timefrane all data collection must be stopped by April 1, 2014, however there is data that continued to be collected long after, however this data is only collected from recidivate criminals and not the non-recidivate criminals. We will fix this error by dropping all of the values that appear after the cut off date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opwx3MQ5elYi"
      },
      "source": [
        "for l in range(len(compas)):\n",
        "  if pd.to_datetime(compas[\"compas_screening_date\"][l]) >= datetime.datetime(2014, 4, 1):\n",
        "    compas = compas.drop([l])\n",
        "print(\"last date of modified dataset: \", compas[\"compas_screening_date\"].max(), \"length of compas: \", len(compas))\n",
        "\n",
        "\n",
        "compasfeat = compas.drop(['id', 'name', 'first', 'last','race','decile_score', 'juv_other_count','v_decile_score','compas_screening_date','dob', 'age_cat', 'days_b_screening_arrest', \n",
        "                          'c_jail_in','c_jail_out','c_case_number', 'c_offense_date', 'c_arrest_date', 'c_days_from_compas','decile_score.1','vr_case_number','vr_charge_degree', \n",
        "                          'vr_offense_date', 'vr_charge_desc','type_of_assessment','score_text', 'r_charge_desc','screening_date', 'v_type_of_assessment', 'v_score_text', 'v_screening_date', \n",
        "                          'c_charge_desc','num_r_cases','r_case_number','r_days_from_arrest','r_offense_date','r_jail_in','r_jail_out','num_vr_cases', 'is_recid','r_charge_degree','is_violent_recid' ], axis=1)\n",
        "\n",
        "\n",
        "print(\"Null values: \", compasfeat.isnull().values.any(), \"Sum: \", compasfeat.isnull().sum())\n",
        "\n",
        "print(len(compasfeat))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgemOiIgfj8j"
      },
      "source": [
        "**Preprocessing**\n",
        "\n",
        "Now we are ready to preprocess the data for the model. \n",
        "First we split the data into features and the target data point.\n",
        "\n",
        "Then we scale the data down so if there is one data point that is much larger than the others it won't skew the data too hard one way or the other. In this case that data is the age feature. all the other values are 0,1 or a value less that 10. However when it comes to age that goes even above 70, which is much higher than the other values. That is the point of scalling the data down, so that age doesn't unfairly influence that data were it shouldn't . \n",
        "\n",
        "Then we print the values of the scaled data as well as the mean as proof of the data's new lower scale.\n",
        "\n",
        "We also create a paramter called \"epoch\"  with the value of 1000 which will be used several times in the upcoming code. This will tell us how many times the run the model in the training stage as well as help us get other necessary values later. \n",
        "\n",
        "In the last step of preprocessing we check for any missing values. In this case there are none, but there are several wauys to handle that scenario if it arises.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dY-SaG4fkQ3"
      },
      "source": [
        "\n",
        "x = np.array(compasfeat)\n",
        "y = np.array(compas[predict])\n",
        "\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "x = scaler.fit_transform(x)\n",
        "\n",
        "epochs= 1000\n",
        "print(\"MEAN OF SCALED DATA: \", x.mean(axis=0))\n",
        "print(\"STANDARD DEVIATION OF SCALED DATA\", x.std(axis=0))\n",
        "print(len(compasfeat))\n",
        "\n",
        "\n",
        "print(\"Null values: \", compasfeat.isnull().values.any(), \"Sum: \", compasfeat.isnull().sum())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_gGH6eKhecT"
      },
      "source": [
        "**The Model**\n",
        "\n",
        "Now we get the meat of the work! here is where we train and test the model.\n",
        "\n",
        "As you can see before the model \"for loop\" starts, there is a variable called \"svmaccuracies\" This is necessary later for calculatiing the confidence interval of the model. \n",
        "Underneath that variable is the variable best, where we will store the highest accuracy achieved by the model.\n",
        "\n",
        "In side the for loop we first split the x and y into train and test categories, with four categories total.\n",
        "\n",
        "Then we create the model by pulling an aleready made stable version of the non linear SVM model from the sklearn repository.\n",
        "\n",
        "next we fit the data to the model and then to get an accuracy we \"Score\" the data and also round the data points up to two decimals which is standard. We then append all of the accuracies that we get into the svmaccuracies variable. \n",
        "\n",
        "underneath that is the \"if-statement\" for finding the highest accuracy achieved by the model. It compares the current accuracy to the highest current accuracy achieved. if the current accuracy is lower than the the highest one achieved at that point, the \"best\" value stays the same, however if it is higher then the best value will be replaced and the model will continue to run. \n",
        "\n",
        "The highest accuracy will be saved by pickling. (the code to pull the saved data back out is in side the triple astericks so the model doesn't have to be run again from the start to get the results you want later)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i28loVbJhenv"
      },
      "source": [
        "print(compasfeat.columns)\n",
        "svmaccuracies= []\n",
        "best = 0\n",
        "for _ in range(1000):\n",
        "    x_train3, x_test3, y_train3, y_test3 = sklearn.model_selection.train_test_split(x, y, test_size=.20)\n",
        "    clf= svm.NuSVC(gamma='auto', probability=True)\n",
        "    clf.fit(x_train3, y_train3)\n",
        "    svmacc =clf.score(x_test3, y_test3)\n",
        "    svmacc = round(svmacc*100, 2)\n",
        "    svmaccuracies.append(svmacc)\n",
        "    if best < svmacc:\n",
        "      best = svmacc\n",
        "      print(best)\n",
        "print(\"Highest Accuracy: \", best)\n",
        "bigdataSVM = 'bigdataSVM.sav'\n",
        "pickle.dump(clf, open(bigdataSVM, 'wb'))\n",
        "\n",
        "#open saved model\n",
        "#openbigdataSVM = pickle.load(open(\"bigdataSVM.sav\", \"rb\"))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_Q6Hb8Dk8Zu"
      },
      "source": [
        "**Results**\n",
        "\n",
        "this is where the svmaccuracies variable importance is shwon. We use that variable to get teh mean, standard diviation, as well as confidence interval of the model. the results are displated at the bottom keep the code neat in a way that i like, but it can be moved anywhere below the variables created to make the stats possible. \n",
        "\n",
        "next we create an i variable with a random number. this is used to get one value of the x_test and y_test datasets that will display all the prisonaer inforamtion.\n",
        "\n",
        "we also print the confusion matrix,the LIME explainer in pyploy figure format, and the AUC for this one instance to show the results.  all of these results are also saved using 'savfig' in pdf format, however there do exists different formats as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH4udZ7tk8mV"
      },
      "source": [
        "#results\n",
        "\n",
        "svmmeans = np.mean(svmaccuracies, 0)\n",
        "svmstds = np.std(svmaccuracies, 0)\n",
        "svmci95 = 1.96*svmstds/np.sqrt(epochs)\n",
        "\n",
        "i = 66\n",
        "print(\"prisoner information:\\n\", x_test3[i],y_test3[i])\n",
        "#SVM confusion matric\n",
        "svm_matrix = confusion_matrix(y_test3, clf.predict(x_test3), normalize='all')\n",
        "explainer3 = lime.lime_tabular.LimeTabularExplainer(x_train3, mode='classification', feature_names = compasfeat.columns.values, class_names=['no recid', 'recid'], verbose=True, discretize_continuous=True, discretizer='quartile')\n",
        "\n",
        "print(\"\\nSupport vector machine accuracy: \", best, \"\\nprediction: \", clf.predict(x_test3[i].reshape(1, -1)), \"\\nPredict probability: \", clf.predict_proba(x_test3[i].reshape(1, -1)), \"\\nConfusion Matrix:\\n\", svm_matrix)\n",
        "\n",
        "print(\"Support Vector Machine: \\n\")\n",
        "svmexp = explainer3.explain_instance(x_test3[i], clf.predict_proba, num_features=len(compasfeat.columns))\n",
        "#svmexp.show_in_notebook(show_table=True)\n",
        "svmimg = svmexp.as_pyplot_figure()\n",
        "svmimg.savefig('bigdataSVMexp', format='pdf',bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "y_pred_proba = clf.predict_proba(x_test3)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test3,  y_pred_proba)\n",
        "auc = round(metrics.roc_auc_score(y_test3, y_pred_proba),2)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.savefig('bigdatasvmAUC.pdf', format='pdf',bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Mean validation accuracy/loss: \", svmmeans, \"stddev: \", svmstds, \"Confidence Interval: \",svmci95)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}